{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 30  # Adjust based on your batch size to reach 30k or 50k batches\n",
    "num_unmasked_pixels = 6  # Change to 4 for the second experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskAllButNPixels(object):\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = transforms.ToTensor()(img).squeeze()\n",
    "        mask = torch.zeros_like(img)\n",
    "        new_img = torch.zeros_like(img)\n",
    "        \n",
    "        non_zero_indices = torch.nonzero(img)\n",
    "        if len(non_zero_indices) < self.N:\n",
    "            return torch.stack([mask, img])\n",
    "        \n",
    "        random_indices = np.random.choice(len(non_zero_indices), size=self.N, replace=False)\n",
    "        rows, cols = non_zero_indices[random_indices].T\n",
    "        \n",
    "        mask[rows, cols] = 1\n",
    "        new_img[rows, cols] = img[rows, cols]\n",
    "        \n",
    "        return torch.stack([mask, new_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MaskAllButNPixels and visualize results for multiple scenarios\n",
    "transform = MaskAllButNPixels(num_unmasked_pixels)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "\n",
    "# Test scenarios\n",
    "scenarios = [\n",
    "    (\"Random digit\", random.randint(0, len(mnist_data) - 1)),\n",
    "    (\"Digit with few non-zero pixels\", mnist_data.targets.tolist().index(1)),  # '1' typically has fewer pixels\n",
    "    (\"Digit with many non-zero pixels\", mnist_data.targets.tolist().index(8))  # '8' typically has more pixels\n",
    "]\n",
    "\n",
    "for scenario_name, sample_idx in scenarios:\n",
    "    sample_image, label = mnist_data[sample_idx]\n",
    "\n",
    "    # Apply the transformation\n",
    "    transformed_image = transform(sample_image)\n",
    "\n",
    "    # Visualize the original, mask, and masked image\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axes[0].imshow(sample_image, cmap='gray')\n",
    "    axes[0].set_title(f'Original Image (Digit: {label})')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(transformed_image[0].squeeze(), cmap='gray')\n",
    "    axes[1].set_title('Mask')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(transformed_image[1].squeeze(), cmap='gray')\n",
    "    axes[2].set_title(f'Masked Image ({num_unmasked_pixels} pixels)')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Scenario: {scenario_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the number of non-zero pixels in the masked image\n",
    "    print(f\"Scenario: {scenario_name}\")\n",
    "    print(f\"Number of non-zero pixels in masked image: {torch.count_nonzero(transformed_image[1])}\")\n",
    "    print(f\"Total non-zero pixels in original image: {torch.count_nonzero(transforms.ToTensor()(sample_image))}\")\n",
    "    print()\n",
    "\n",
    "# Test edge case: image with fewer non-zero pixels than num_unmasked_pixels\n",
    "edge_case_image = torch.zeros((28, 28))\n",
    "edge_case_image[14, 14] = 1  # Single pixel in the center\n",
    "edge_case_transformed = transform(transforms.ToPILImage()(edge_case_image))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(edge_case_image, cmap='gray')\n",
    "axes[0].set_title('Edge Case: Single Pixel Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(edge_case_transformed[0].squeeze(), cmap='gray')\n",
    "axes[1].set_title('Mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(edge_case_transformed[1].squeeze(), cmap='gray')\n",
    "axes[2].set_title(f'Masked Image ({num_unmasked_pixels} pixels)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Edge Case: Fewer non-zero pixels than num_unmasked_pixels\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Edge Case: Single Pixel Image\")\n",
    "print(f\"Number of non-zero pixels in masked image: {torch.count_nonzero(edge_case_transformed[1])}\")\n",
    "print(f\"Total non-zero pixels in original image: {torch.count_nonzero(edge_case_image)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\n",
    "Concretely, the judge is trained to classify MNIST from 6 (resp. 4) nonzero pixels, with the pixels\n",
    "chosen at random at training time. The judge receives two input feature planes:\n",
    "1. A {0, 1} mask of which pixels were revealed\n",
    "2. The value of the revealed pixels (with zeros elsewhere)\n",
    "\n",
    "We used the architecture from the TensorFlow MNIST layers tutorial;\n",
    "the only difference is the input. We train the judges using:\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 10^-4\n",
    "- Batches: 30k (resp. 50k)\n",
    "- Batch size: 128 samples\n",
    "\n",
    "Accuracy achieved:\n",
    "- 6 pixels: 59.4%\n",
    "- 4 pixels: 48.2%\n",
    "\"\n",
    "\n",
    "https://web.archive.org/web/20180516102820/https://www.tensorflow.org/tutorials/layers#building_the_cnn_mnist_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 1024)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x)) # 1. Convolutional Layer #1: Applies 32 5x5 filters (extracting 5x5-pixel subregions), with ReLU activation function\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2) # 2. Pooling Layer #1: Performs max pooling with a 2x2 filter and stride of 2 (which specifies that pooled regions do not overlap)\n",
    "        x = torch.relu(self.conv2(x)) # 3. Convolutional Layer #2: Applies 64 5x5 filters, with ReLU activation function\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2) # 4. Pooling Layer #2: Again, performs max pooling with a 2x2 filter and stride of 2\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x)) # 5. Dense Layer #1: 1,024 neurons, with dropout regularization rate of 0.4 (probability of 0.4 that any given element will be dropped during training)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) # 6. Dense Layer #2 (Logits Layer): 10 neurons, one for each digit target class (0â€“9).\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
